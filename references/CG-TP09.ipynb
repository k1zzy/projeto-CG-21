{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17c9f67b-8363-4841-8c6e-3377c12ec215",
   "metadata": {},
   "source": [
    "# Computa√ß√£o Gr√°fica 2025/2026\n",
    "## Aula TP09 - Introdu√ß√£o ao Ray Casting e Ray Tracing\n",
    "\n",
    "### Um tutorial gr√°fico por Andr√© Falc√£o (2025)\n",
    "\n",
    "Nesta aula vamos aprender como podemos representar imagens mais realistas. Come√ßaremos com o Ray Casting e depois iremos para o Ray Tracing. Estes algoritmos ser√£o descritos em detalhe, e incluem toda a mat√©ria discutida at√© agora\n",
    "\n",
    "1. Tracing de raios e lidar com intersec√ß√µes\n",
    "2. Algoritmo de Ray Casting\n",
    "3. Algoritmo de Ray Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd965285-8757-464b-a864-f0f547c9b0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cffe8b-9e23-4292-97c5-7b6f7b235603",
   "metadata": {},
   "source": [
    "## 1. Fun√ß√µes b√°sicas\n",
    "\n",
    "Antes de implementar qualquer dos algoritmos, precisamos de resolver duas quest√µes b√°sicas. a) como podemos descobrir os elementos que aparecem quando um raio √© emitido; e b) dado um raio, quais as intersec√ß√µes que temos na cena. Como faremos uma abordagem bottom-up criaremos as fun√ß√µes antes de serem chamadas no c√≥digo\n",
    "\n",
    "### 1.1. Lidar com intersec√ß√µes\n",
    "\n",
    "A primeira coisa que necessitaremos √© de uma fun√ß√£o para detectar intersec√ß√µes. Se temos um raio com um ponto de origem e um vector, a pergunta que queremos responder √©, onde √© que esse vector interesecciona esse objecto\n",
    "\n",
    "Como a nossa cena ser√£o s√≥ esferas, a fun√ß√£o de intersec√ß√£o pode ser muito simplificada, dado que, se sabemos o  centro  e o raio s√≥ temos que verificar se o raio chega √† esfera, e se sim, qual a dist√¢ncia a que est√° o ponto de intersec√ß√£o  \n",
    "\n",
    "Para  \n",
    "* Raio: $r(t) = \\mathbf{ro} + t.\\mathbf{rd}$, onde **ro** √© a origem e **rd** √© a direc√ß√£o (normalizada).\n",
    "* Esfera: centro **c** e raio **r**.\n",
    "* Procuramos o menor $t > 0$ tal que o ponto do raio est√° na superf√≠cie da esfera.\n",
    "\n",
    "\n",
    "A condi√ß√£o de intersec√ß√£o √©:\n",
    "$$ \\|\\mathbf{ro} + t\\,\\mathbf{rd} - \\mathbf{c}\\|^2 = r^2. $$\n",
    "\n",
    "Definindo $\\mathbf{oc} = \\mathbf{ro}-  \\mathbf{c}$:\n",
    "\n",
    "$$ \\|\\mathbf{oc} + t\\,\\mathbf{rd}\\|^2 = r^2 \\;\\Longrightarrow\\; (\\mathbf{rd}\\cdot\\mathbf{rd})\\,t^2 + 2(\\mathbf{oc}\\cdot\\mathbf{rd})\\,t + (\\mathbf{oc} \\cdot \\mathbf{oc} - r^2) = 0. $$\n",
    "\n",
    "Se $\\mathbf{rd}$ est√° normalizado, ent√£o $\\mathbf{rd} \\cdot \\mathbf{rd}=1$ e ficamos com a forma:\n",
    "\n",
    "$$ t^2 + 2b\\,t + c = 0, $$\n",
    "\n",
    "onde\n",
    "$$ b = \\mathbf{oc}\\!\\cdot\\!\\mathbf{rd}, \\qquad c = \\mathbf{oc}\\!\\cdot\\!\\mathbf{oc} - r^2. $$\n",
    "\n",
    "As ra√≠zes desta forma simplificada da equa√ß√£o quadr√°tica s√£o:\n",
    "\n",
    "$$  t = -b \\pm \\sqrt{\\,b^2 - c\\,} $$\n",
    "\n",
    "\n",
    "E escolhemos o valor menor.\n",
    "\n",
    "O c√≥digo em baixo √© uma implementa√ß√£o directa desta an√°lise:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a17aee4-48ac-4d5a-9e7e-05b0ebcca5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect_sphere(ro, rd, c, r):\n",
    "    oc = ro - c\n",
    "    b = np.dot(oc, rd)\n",
    "    cval = np.dot(oc, oc) - r*r\n",
    "    disc = b*b - cval\n",
    "    if disc < 0: return np.inf\n",
    "    s = np.sqrt(disc)\n",
    "    t1, t2 = -b - s, -b + s\n",
    "    \n",
    "    #vamos escolher apenas dist√¢ncias positivas e suficientemente distantes do observador (da√≠ o threshold de 0.0001)\n",
    "    # eo valor √© o menor poss√≠vel\n",
    "    t = t1 if (t1 > 1e-4) else (t2 if t2 > 1e-4 else np.inf)\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e2749f-0bbb-4689-be17-987d6a05b9c9",
   "metadata": {},
   "source": [
    "### 1.2. Emiss√£o de Raios\n",
    "\n",
    "A seguir necessitaremos de uma fun√ß√£o em que, dado um ponto (ro) e um vector (rd), no espa√ßo da cena devolve o ponto que encontrou, usando naturalmente a fun√ß√£o de intersec√ß√£o anterior\n",
    "\n",
    "O retorno √©:\n",
    "* `hit` - Ponto em que o raio atingiu o objecto\n",
    "* `idx` - identificador do objecto (forma ou tri√°ngulo)\n",
    "* `t` - componente de `rd` que aplicado a `ro` d√° o `hit`: $ hit=ro+t.rd $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2bd0d2-de7b-4ef6-8891-34111aa34f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace(ro, rd):\n",
    "    t_min, hit, idx = np.inf, None, -1\n",
    "    for i, (c, r, col, shin, refl) in enumerate(spheres):\n",
    "        t = intersect_sphere(ro, rd, c, r)\n",
    "        if t < t_min:\n",
    "            t_min, hit, idx = t, ro + t*rd, i\n",
    "    return hit, idx, t_min\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a9ea14-3472-4b9b-9399-3530241de857",
   "metadata": {},
   "source": [
    "## 2. Algoritmo de Ray Casting\n",
    "\n",
    "O Ray Casting tem uma forma muito simples de ser processado. √â assim\n",
    "\n",
    "1. Varre os v√°rios pixeis definidos no viewport, sabendo as coordenadas (ro inicial) e orienta√ß√£o da c√¢mara\n",
    "2. Para cada pixel x,y, determina o seu vector (rd) criando um \"raio\" que percorre a cena, determinando o objecto que lhe est√° mais pr√≥ximo\n",
    "3. Para esse objecto, aplica-lhe a fun√ß√£o de ilumina√ß√£o apropriada de acordo com as suas propriedades\n",
    "\n",
    "Uma implementa√ß√£o directa destes princ√≠pios pode ser vista na fun√ß√£o seguinte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157f7dc6-3bfb-4669-849e-cec07f30be71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shade_ray_casting(ro, rd):\n",
    "    #Identificar em que √© que se embate - se nada, √© o background e √© o fim\n",
    "    hit, idx, t = trace(ro, rd)\n",
    "    if hit is None:\n",
    "        return bg\n",
    "\n",
    "    #recolher a info do material do objecto atingido (ver as esferas j√° a seguir)\n",
    "    c, r, base_col, shin, refl = spheres[idx]\n",
    "    n = (hit - c) / r #calcular a normal\n",
    "    n = n / np.linalg.norm(n)  # Ensure normalized\n",
    "    \n",
    "    # calcular a posi√ß√£o da luz relativamente ao ponto e dist√¢ncia\n",
    "    to_light = light_pos - hit\n",
    "    dist_l = np.linalg.norm(to_light)\n",
    "    ldir = to_light / dist_l\n",
    "\n",
    "    ambient = 0.1\n",
    "    diff=max(0.0, np.dot(n, ldir))\n",
    "    h = (ldir - rd) / np.linalg.norm(ldir - rd)\n",
    "    spec=max(0.0, np.dot(n, h)) ** shin\n",
    "    final_color = base_col * (ambient + 0.9*diff) + light_col * (0.3*spec)\n",
    "    \n",
    "    return np.clip(final_color, 0, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9ab671-743f-478e-9697-64db64d4e62b",
   "metadata": {},
   "source": [
    "### 2.2. Definir a cena\n",
    "\n",
    "Vamos definir a cena apenas com 4 esferas. Uma delas de raio 1000 e que servir√° como \"ch√£o\"\n",
    "\n",
    "Define-se ainda uma luz apenas com posi√ß√£o e cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fd931b-5f71-45b0-bee9-cb4a9a65ef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#centro, raio, cor, especularidade\n",
    "spheres = [\n",
    "    (np.array([ 0.0, -0.3, 0.0]), 0.4, np.array([0.9, 0.2, 0.2]), 50, 0.3),  # ligeiramente reflectiva vermelha\n",
    "    (np.array([ 1.1, -0.2, 0.3]), 0.3, np.array([0.2, 0.9, 0.2]), 50, 0.0),  # Verde sem reflex√£o\n",
    "    (np.array([ 0.6,  0.3, 0.5]), 0.25, np.array([0.8, 0.8, 0.9]), 200, 0.8), # Esfera Met√°lica\n",
    "    (np.array([ 0.0, -1000.5, -2.0]), 1000.0, np.array([0.9, 0.9, 0.95]), 8, 0.1) # ch√£o ligeiramente reflexivo\n",
    "]\n",
    "\n",
    "#Luz\n",
    "light_pos = np.array([2.1, .5, 0.5])\n",
    "light_col = np.array([1.0, 1.0, 1.0])\n",
    "\n",
    "#cor de fundo <- azul c√©u\n",
    "bg = np.array([0.7, 0.85, 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908dd617-0fa5-42f9-a29d-acd6529f7398",
   "metadata": {},
   "source": [
    "\n",
    "Os par√¢metros da cena s√£o definidos em baixo com uma imagem de 640x480,  fov vertical de 60 graus e posi√ß√£o da c√¢mara em $[0, 0.1, 3.0])$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd8cf44-c837-4bcd-94df-d558600b2d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "W, H = 640, 480\n",
    "fov = 60 * np.pi/180\n",
    "cam_pos = np.array([0, 0.1, 3.0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab75a98-f68a-4422-b2dd-41edc6b1b015",
   "metadata": {},
   "source": [
    "### 2.3. Defini√ß√£o da c√¢mara\n",
    "\n",
    "O setup da c√¢mara √© uma invers√£o do LookAt (que j√° conhecemos) retornando os vectores forward, right e up para processamento na parte de produ√ß√£o de raios\n",
    "\n",
    "**Deriva√ß√£o 1: $V^{-1}$ a partir de $V$**\n",
    "\n",
    "A **view matrix** leva coordenadas do **mundo** para **c√¢mara**:\n",
    "$$\n",
    "V=\\begin{bmatrix}\n",
    "R^\\top & -R^\\top \\mathbf{eye}\\\\\n",
    "\\mathbf{0}^\\top & 1\n",
    "\\end{bmatrix},\n",
    "\\qquad\n",
    "\\begin{bmatrix}\\mathbf{x}\\\\1\\end{bmatrix}\n",
    "=\n",
    "V\\begin{bmatrix}\\mathbf{p}\\\\1\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "R^\\top(\\mathbf{p}-\\mathbf{eye})\\\\[2pt]\n",
    "1\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Logo, em coordenadas 3D n√£o-homog√©neas, que transforma qualquer ponto em coordenadas do mundo em coordenadas da c√¢mara (composi√ß√£o de uma rota√ß√£o e de uma transla√ß√£o)\n",
    "$$\n",
    "\\mathbf{x}=R^\\top(\\mathbf{p}-\\mathbf{eye}).\n",
    "$$\n",
    "\n",
    "Isolando $p$:\n",
    "$$\n",
    "\\mathbf{p}-\\mathbf{eye}=R\\,\\mathbf{x}\n",
    "\\;\\;\\Longrightarrow\\;\\;\n",
    "\\mathbf{p}=R\\,\\mathbf{x}+\\mathbf{eye}.\n",
    "$$\n",
    "\n",
    "Em coordenadas homog√©neas, isto corresponde a aplicar a inversa:\n",
    "$$\n",
    "\\begin{bmatrix}\\mathbf{p}\\\\1\\end{bmatrix}\n",
    "=\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "R & \\mathbf{eye}\\\\\n",
    "\\mathbf{0}^\\top & 1\n",
    "\\end{bmatrix}}_{\\displaystyle V^{-1}}\n",
    "\\begin{bmatrix}\\mathbf{x}\\\\1\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Esta matriz $V^{-1}$ √© muito semelhante √† matriz V e pode ser produzida com um processo id√™ntico, recebendo os mesmos par√¢metros de entrada. Na nossa fun√ß√£o, por raz√µes pr√°ticas, vamos apenas fazer sair os vectores `forward`, `right` e `up`\n",
    "\n",
    "\n",
    "#### Exerc√≠cio\n",
    "* Ver na TP correspondente como foi calculada a Matriz LookAt ($V$) e comparar com esta e com a forma como √© aplicada\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c846f5-c13b-4332-bacd-f437684b5dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def setup_camera(eye, target, up):\n",
    "    #simplifica√ß√£o do LookAt\n",
    "    forward = target - eye\n",
    "    forward = forward / np.linalg.norm(forward)\n",
    "    \n",
    "    # Vector right\n",
    "    right = np.cross(forward, up)\n",
    "    right = right / np.linalg.norm(right)\n",
    "    \n",
    "    # garantir vectores ortogonais\n",
    "    up = np.cross(right, forward)\n",
    "    up = up / np.linalg.norm(up)\n",
    "    \n",
    "    return forward, right, up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7599eb00-0d7d-4e5b-a99e-f9be232bc5b6",
   "metadata": {},
   "source": [
    "### 2.4. Deriva√ß√£o de `px` e `py` para Ray Casting (pinhole invertido)\n",
    "\n",
    "Agora que sabemos como transformar qualquer ponto de coordenadas da c√¢mara para coordenadas do mundo, vamos obter esses pontos, varrendo o ecr√£. Par isso vamos precisar das seguintes fases:\n",
    "1. Obter a base das coordenadas da c√¢mara\n",
    "2. Inverter a fun√ß√£o perspectiva usando o plano de imagem e o Campo de vis√£o (FOV)\n",
    "3. Calcular as coordenadas normalizadas do pixel\n",
    "4. Calcular cada ponto correspondente no plano da imagem\n",
    "\n",
    "\n",
    "**1. Defini√ß√£o da base ortonormada da c√¢mara**\n",
    "Seja uma base ortonormal da c√¢mara obtida pela fun√ß√£o anterior:\n",
    "- **forward**: $ \\mathbf{f} $\n",
    "- **right**: $ \\mathbf{r} $\n",
    "- **up**: $ \\mathbf{u} $\n",
    "\n",
    "Assume-se $ \\{\\mathbf{r},\\mathbf{u},\\mathbf{f}\\} $ ortonormais e com orienta√ß√£o de m√£o direita.\n",
    "\n",
    "**2. Inverter a fun√ß√£o perspectiva**\n",
    "\n",
    "Coloca-se o **plano da imagem** a uma dist√¢ncia focal $f$ do centro de c√¢mara, ao longo de $ \\mathbf{f} $.\n",
    "Para **FOV vertical** (fovy), a meia-altura do plano √©:\n",
    "$$\n",
    "h = f\\,\\tan\\!\\left(\\frac{\\mathrm{fov}}{2}\\right),\n",
    "$$\n",
    "e a **meia-largura** √©\n",
    "$$\n",
    "w = h \\cdot \\mathrm{aspect}, \\quad \\text{com } \\mathrm{aspect} = \\frac{W}{H}.\n",
    "$$\n",
    "\n",
    "NOTA: se fizermos $f=1$ simplificamos as f√≥rmulas, como veremos a seguir\n",
    "\n",
    "**3. Calcular as coordenadas normalizadas do pixel (NDC)**\n",
    "\n",
    "Para o pixel **inteiro** $(x,y)$, com $x\\in[0,W-1]$ e $y\\in[0,H-1]$, usa-se o **centro do pixel** com $+0.5$ e mapeia-se para $(u,v)\\in[-1,1]\\times[-1,1]$:\n",
    "\n",
    "O eixo $x$ (esquerda ‚Üí direita) fica assim:\n",
    "$$ u = \\left(\\frac{x+0.5}{W}\\right)\\cdot 2 - 1. $$\n",
    "\n",
    "O eixo $y$ (topo ‚Üí fundo) √© ligeiramente diferente. Como a imagem tem $y$ a crescer para baixo, mas queremos $+v$ a apontar para **cima**, inverte-se:\n",
    "$$ v = 1 - \\left(\\frac{y+0.5}{H}\\right)\\cdot 2. $$\n",
    "\n",
    "**4.Projec√ß√£o do ponto no plano da imagem**\n",
    "\n",
    "Isto √© o que queremos obter, que √©, como cada pixel projectado entre $[-1,1]$, se pode converter para coordenadas f√≠sicas no plano da imagem, que com $f=1$:\n",
    " fica:\n",
    "\n",
    "$$ x_{\\text{img}} =  u \\cdot w = u \\cdot \\mathrm{aspect}\\cdot \\tan\\!\\left(\\frac{\\mathrm{fov}}{2}\\right), \\quad\n",
    "y_{\\text{img}} = v \\cdot h = v \\cdot \\tan\\!\\left(\\frac{\\mathrm{fov}}{2}\\right). $$\n",
    "\n",
    "\n",
    "Que correspondem aos pixeis x e y em **cordenadas de c√¢mara.** A aplica√ß√£o dos vectores da fun√ß√£o de projec√ß√£o (provenientes do setup_camera) projecta essas coordenadas em coordenadas do mundo, e assim conseguimos criar o vector `rd` para definir o raio inicial juntamente com as coordenadas da c√¢mara"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d756fd78-0e38-4689-a2fb-09662deb0316",
   "metadata": {},
   "source": [
    "### 2.5. A fun√ß√£o principal de Ray Casting\n",
    "\n",
    "A fun√ß√£o de ray casting √© a fun√ß√£o principal e, depois de definir os par√¢metros da c√¢mara, varre cada um dos pixeis, invertendo a fun√ß√£o de perspectiva para calcular a posi√ß√£o dos pixeis e depois definir o vector (ray direction - `rd`) com a direc√ß√£o. O ponto de origem (ray origin `ro`) √© a posi√ß√£o da c√¢mara\n",
    "\n",
    "Esta fun√ß√£o salva uma imagem em PNG e devolve essa mesma imagem para ser mostrada no ecr√£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54686972-eb9c-475b-b0a0-1cf9b4b7cb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ray_cast(fname=\"ray_cast.png\"):\n",
    "    cam_target = np.array([0, 0, 0])     # Point camera is looking at\n",
    "    cam_up = np.array([0, 1, 0])         # Up direction\n",
    "    aspect = W / H\n",
    "    cam_forward, cam_right, cam_UP = setup_camera(cam_pos, cam_target, cam_up)\n",
    "    img = np.zeros((H, W, 3), dtype=np.float32)\n",
    "    for y in range(H):\n",
    "        for x in range(W):\n",
    "            #posi√ß√£o do Pixel (x,y)\n",
    "            px = ((x + 0.5) / W - 0.5) * 2.0 * aspect * np.tan(fov/2)\n",
    "            py = ((H - y - 0.5) / H - 0.5) * 2.0 * np.tan(fov/2)\n",
    "\n",
    "            #agora projectar para coordenadas do mundo e normalizar\n",
    "            rd = px * cam_right + py * cam_UP + cam_forward\n",
    "            rd = rd / np.linalg.norm(rd)\n",
    "            \n",
    "            # e aqui est√° o procedimento final<- determina√ß√£o da cor atrav√©s do ray casting.\n",
    "            color = shade_ray_casting(cam_pos, rd) \n",
    "            img[y, x] = color\n",
    "\n",
    "    Image.fromarray((np.clip(img,0,1)*255).astype(np.uint8)).save(fname)\n",
    "    print(\"Guardado:\"+fname)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea895ba-e3e0-4e68-9667-39e8471515f6",
   "metadata": {},
   "source": [
    "Podemos agora criar uma imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d419e4c1-60fb-4266-a49f-056c056f6a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = ray_cast()\n",
    "plt.imshow(img, clim=[0, 1])  \n",
    "plt.axis('off') \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e904a14d-6808-4905-8bd5-b9f522dcb5f8",
   "metadata": {},
   "source": [
    "## 3. Ray tracing\n",
    "\n",
    "O algoritmo de Ray-tracing √© muito semelhante ao de Ray Casting envolvendo mais 2 componentes. H√° uma 3a componente (transmiss√£o em objectos transparentes) que n√£o ser√° tratada aqui.\n",
    "\n",
    "Vamos aproveitar quase tudo o que temos do ray casting pois muitos dos pric√≠pios s√£o os mesmos, pelo que n√£o necessitaremos de redifinir a detec√ß√£o de colis√µes de raios nem de recriar a cena e as suas propriedades\n",
    "\n",
    "1. Varre os v√°rios pixeis definidos no viewport, sabendo as coordenadas (ro inicial) e orienta√ß√£o da c√¢mara\n",
    "2. Para cada pixel x,y, determina o seu vector (rd) criando um \"raio\" que percorre a cena, determinando o objecto que lhe est√° mais pr√≥ximo\n",
    "3. Para esse objecto e ponto de contacto cria um raio novo em direc√ß√£o √† fonte de ilumina√ß√£o e,\n",
    "   1. caso n√£o haja nada no caminho, aplica-lhe a fun√ß√£o de ilumina√ß√£o apropriada de acordo com as suas propriedades\n",
    "   2. Caso exista algum obst√°culo, aplica-lhe a luz ambiente\n",
    "4. Para esse mesmo ponto, caso ele tenha alguma reflectividade, vai ver a cor associada aos raios que podem ter atingido esse ponto, propagando recursivamente este algoritmo de acordo com a condi√ß√£o de paragem\n",
    "5. [**N√£o vista nesta aula**] Caso o objecto tenha alguma transpar√™ncia vai fazer um raio atrav√©s das propriedades transmissivas do objecto e propag√°-lo para a cena recursivamente de acordo com a condi√ß√£o de paragem\n",
    "\n",
    "A fun√ß√£o `shade_ray_tracing` inclui essas modifica√ß√µes, sendo no restante id√™ntica √† fun√ß√£o `shade_ray_casting` De notar que existe um par√¢metro extra de profundidade (`depth`) que controla a profundidade de recurs√£o do algoritmo e uma nova vari√°vel global de controlo que define esses limites\n",
    "\n",
    "A fun√ß√£o auxiliar  `reflect`,  tamb√©m definida aqui, d√° a direc√ß√£o de reflex√£o dado um vector de raio e a normal do objecto embatido e ser√° √∫til a seguir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3572833-6c8e-4030-8e57-0137f835fe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reflect(rd, n):\n",
    "    # Calcula a direc√ß√£o de reflex√£o: rd - 2*(rd¬∑n)*n\n",
    "    return rd - 2 * np.dot(rd, n) * n\n",
    "\n",
    "\n",
    "def shade_ray_tracing(ro, rd, depth=0):\n",
    "    # Fim da recurs√£o se atingimos o limite\n",
    "    if depth > MAX_DEPTH:\n",
    "        return bg * 0.1  #background atenuado\n",
    "\n",
    "    #Identificar em que √© que se embate - se nada, √© o background e √© o fim\n",
    "    hit, idx, t = trace(ro, rd)\n",
    "    if hit is None:\n",
    "        return bg\n",
    "\n",
    "    #recolher a info do material do objecto atingido\n",
    "    c, r, base_col, shin, refl = spheres[idx]\n",
    "    n = (hit - c) / r #calcular a normal\n",
    "    n = n / np.linalg.norm(n)  # Ensure normalized\n",
    "    \n",
    "    # calcular a posi√ß√£o da luz relativamente ao ponto e dist√¢ncia\n",
    "    to_light = light_pos - hit\n",
    "    dist_l = np.linalg.norm(to_light)\n",
    "    ldir = to_light / dist_l\n",
    "\n",
    "    #e ver se nada est√° no caminho da(s) luz(es)\n",
    "    eps = 1e-3\n",
    "    shadow_hit, idx2, tshadow = trace(hit + n*eps, ldir)\n",
    "    in_shadow = (shadow_hit is not None) and (tshadow < dist_l - 1e-3)\n",
    "\n",
    "    #e define-se a cor local (Phong)\n",
    "    ambient = 0.1\n",
    "    if in_shadow:\n",
    "        diff = 0.0 \n",
    "        spec = 0.0\n",
    "    else: \n",
    "        diff=max(0.0, np.dot(n, ldir))\n",
    "        h = (ldir - rd) / np.linalg.norm(ldir - rd)\n",
    "        spec=max(0.0, np.dot(n, h)) ** shin\n",
    "    local_color = base_col * (ambient + 0.9*diff) + light_col * (0.3*spec)\n",
    "    \n",
    "    # Se o material √© reflexivo, Faz-se a recurs√£o\n",
    "    if refl > 0:\n",
    "        reflect_dir = reflect(rd, n)\n",
    "        reflect_color = shade_ray_tracing(hit + n*eps, reflect_dir, depth + 1)\n",
    "        # finalmente faz o Blending da cor local com a cor proveniente dos reflexos\n",
    "        final_color = local_color * (1 - refl) + reflect_color * refl\n",
    "    else:\n",
    "        final_color = local_color\n",
    "    \n",
    "    return np.clip(final_color, 0, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb721d51-cb10-4e84-b89a-bf321d90f41b",
   "metadata": {},
   "source": [
    "### 3.1. O ponto de entrada\n",
    "\n",
    "Os princ√≠pios s√£o os mesmos do c√≥digo anterior, mas aten√ß√£o ao par√¢matro `MD` que define desde logo qual a profundidade m√°xima a que o algoritmo correr√°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47efa6d1-9712-490c-9b8c-d7ac6a33a13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_DEPTH = 4  # Valor m√°ximo da recurs√£o\n",
    "\n",
    "def ray_trace(fname=\"ray_trace.png\", MD=0):\n",
    "    global MAX_DEPTH\n",
    "    MAX_DEPTH = MD\n",
    "    cam_target = np.array([0, 0, 0])     # Point camera is looking at\n",
    "    cam_up = np.array([0, 1, 0])         # Up direction\n",
    "    aspect = W / H\n",
    "    cam_forward, cam_right, cam_UP = setup_camera(cam_pos, cam_target, cam_up)\n",
    "    img = np.zeros((H, W, 3), dtype=np.float32)\n",
    "    for y in range(H):\n",
    "        for x in range(W):\n",
    "            #posi√ß√£o do Pixel (x,y)\n",
    "            px = ((x + 0.5) / W - 0.5) * 2.0 * aspect * np.tan(fov/2)\n",
    "            py = ((H - y - 0.5) / H - 0.5) * 2.0 * np.tan(fov/2)\n",
    "\n",
    "            #defini√ß√£o do Raio de acordo com os par√¢metros calculados\n",
    "            rd = px * cam_right + py * cam_UP + cam_forward\n",
    "            rd = rd / np.linalg.norm(rd)\n",
    "            # e aqui est√° tudo<- determina√ß√£o da cor atrav√©s do ray tracing.\n",
    "            color = shade_ray_tracing(cam_pos, rd) \n",
    "            img[y, x] = color\n",
    "\n",
    "    Image.fromarray((np.clip(img,0,1)*255).astype(np.uint8)).save(fname)\n",
    "    print(\"Guardado:\"+fname)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfc6482-6ed4-4f82-b586-6bf1349fdbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = ray_trace(fname=\"ray_trace4.png\", MD=4)\n",
    "plt.imshow(img, clim=[0, 1])  \n",
    "plt.axis('off')  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc47448b-2349-42ae-9c9e-b7ab718a34fa",
   "metadata": {},
   "source": [
    "\n",
    "# Exerc√≠cios\n",
    "1. Corra o ray tracing para `MAX_DEPTH` entre 0 e 4 ($[0,1,2,3,4]$) guardando em ficheiros distintos cada uma das imagens\n",
    "   1. Compare os resultados entre si e com a imagem calculada com ray casting\n",
    "   2. Compare os tempos de gera√ß√£o de cada imagem (incluindo o ray casting) e fa√ßa um gr√°fico (fun√ß√£o `plt.plot(x,y)` do matplotlib) mostrando a evolu√ß√£o. Comente os resultados\n",
    "   3. Discuta. Vale a pena (nesta cena) continuar a recurs√£o?\n",
    "2. Volte a executar `ray_trace(fname=\"ray_trace0.png\", MD=0)`\n",
    "   1. O que est√° a acontecer aqui?\n",
    "   2. H√° recurs√£o?\n",
    "   3. Se n√£o h√° recurs√£o porque √© que n√£o √© id√™ntico ao ray casting e se acha que h√° recurs√£o discuta como.\n",
    "   4. Qual a grande vantagem e import√¢ncia desta par√¢metriza√ß√£o e o que consegue fazer aqui (e com MAX DEPTH>0) que n√£o era at√© agora poss√≠vel. Identifique no algoritmo e no c√≥digo onde √© que o efeito aqui produzido acontece e comente-o\n",
    "3. Compare a fun√ß√£o de ilumina√ß√£o dada aqui (e comum a ambos os algoritmos) e compare-a com outros m√©todos e parametriza√ß√µes que j√° vimos nas aulas\n",
    "   1. identifique as grandes diferen√ßas\n",
    "   2. Consegue melhorar o modelo de ilumina√ß√£o (modificando necessariamente a estrutura dos objectos) para que o modelo fique mais parecido com o que se viu nas aulas de Modelos de Ilumina√ß√£o Locais?\n",
    "4. Adicione mais duas esferas √† cena - uma mais pequena, amarela dourada √† frente de todas, e outra maior, azul, atras de todas. Fa√ßa o rendering da cena e controle o tempo de execu√ß√£o. Discuta os resultados\n",
    "5. Coloque a c√¢mara em cima a olhar para baixo (ligeiramente obl√≠qua)\n",
    "6. [PARA FAZER na AULA] ...\n",
    "7. Discuta como poderia ser feito este processo para malhas poligonais. Quais os principais desafios?\n",
    "   1. [PARA DISCUTIR NA AULA] ...\n",
    "8. [OPCIONAL] O c√≥digo em baixo consegue ler de um conjunto de PNGs gerados e criar uma anima√ß√£o.\n",
    "   1. Crie um ciclo em que  se modifique um ou mais dos par√¢metros da cena (por exemplo a posi√ß√£o da c√¢mara, da luz, ou de uma das esferas, ou todas)\n",
    "   2. para cada itera√ß√£o do ciclo gere uma imagem PNG numa pasta separada, como acima, mas numerada. e.g. (`img000.png`, `img001.png`, ...)\n",
    "   3. Crie o filme com o c√≥digo abaixo (aten√ß√£o √† instala√ß√£o do `imageio`)\n",
    "   4. Compartilhe o resultado no forum de alunos! - Ter√° um bonus na nota o melhor video! üôÇ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db09848-69f1-4bfd-990c-d782710083eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aten√ß√£o: Primeiro fazer: pip install imageio imageio[ffmpeg]\n",
    "# o c√≥digo pode dar um aviso devido √†s dimens√µes da janela, mas o video √© gerado\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "png_files=sorted(glob.glob(\"PNGS/*.png\"))\n",
    "fps=30\n",
    "mp4_name = \"ray-tracing-good.mp4\"\n",
    "print(\"A escrever v√≠deo MP4...\")\n",
    "with imageio.get_writer(mp4_name, fps=fps, codec=\"libx264\", quality=8) as w:\n",
    "    for f in png_files:\n",
    "        w.append_data(imageio.imread(f))\n",
    "print(f\"V√≠deo escrito: {mp4_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "panel-cell-order": [
   "71b2794e-2e62-4b2c-8149-2891573b4669"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
